{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3c56c3-1070-45b3-b032-ba974471d165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader, JSONLoader\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "file_path='data.json'\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a02a65-1a70-429d-8987-9b64c38db621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers():\n",
    "\n",
    "    \"\"\"Fetches papers from the arXiv API and returns them as a list of strings.\"\"\"\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=ti:llama&start=0&max_results=70'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read().decode('utf-8')\n",
    "    root = ET.fromstring(data)\n",
    "    papers_list = []\n",
    "\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry' ):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "        summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "        paper_info = f\"Title: {title}\\n Summary: {summary}\\n\"\n",
    "        info = {}\n",
    "        info['Title'] = title\n",
    "        info['Summary'] = summary\n",
    "        papers_list.append(info)\n",
    "    # Check if the list has some content\n",
    "    dic = {}\n",
    "    dic['content'] = papers_list\n",
    "    \n",
    "    if papers_list != []:\n",
    "        with open('data.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(dic, f, ensure_ascii=False)\n",
    "    #if papers_list != []:\n",
    "    #    file = open('data.txt','w')\n",
    "    #    for item in papers_list:\n",
    "    #        file.write(item)\n",
    "    #    file.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7226eda9-6ec1-49fe-baee-c84b03d964ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658699cf-890a-4ad4-a389-5920b467c97e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/christiangrech/Documents/GitHub/Retrieval Augmented Generation - LLAMA/data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nh/8t_4468j4833crt_5nppc45c0000gp/T/ipykernel_28237/3414396628.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     text_content=False)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/langchain/document_loaders/json_loader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0mOpen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mclose\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \"\"\"\n\u001b[0;32m-> 1236\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         return io.open(self, mode, buffering, encoding, errors, newline,\n\u001b[0m\u001b[1;32m   1223\u001b[0m                        opener=self._opener)\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/christiangrech/Documents/GitHub/Retrieval Augmented Generation - LLAMA/data.json'"
     ]
    }
   ],
   "source": [
    "#data = json.loads(Path(file_path).read_text())\n",
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema='.content[]',\n",
    "    text_content=False)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0305c-d156-42ba-a927-e7caac2acb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "293820ff-914a-4691-857b-e307b6f9b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader = TextLoader(\"./data.txt\")\n",
    "#docs = loader.load()\n",
    "\n",
    "# Split text and create vector embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f32bb003-bac1-42aa-970d-29cba0d0bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Name at least 5 domain-specific LLMs that have been created by fine-tuning Llama-2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "847472c5-02a9-4d6e-aca9-6f1eb5452735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The retrieved context does not provide specific information about the model structure of Llama-2, including the required memory, required computing capacity, number of parameters, or available quantizations.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What can you find out about the model structure of Llama-2 (required memory, required computing capacity, number of parameters, available quantizations)? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9d7f58e-c5bc-4699-8b39-61ae8b083a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PUMA is a framework designed to enable fast and secure inference for Transformer models. It reduces the cost of secure inference while maintaining model performance and incorporates secure procedures for Embedding and LayerNorm. PUMA has been open-sourced on the Github repository of SecretFlow-SPU.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is PUMA? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8202c16-c157-45f4-8795-a8eab4f263c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
