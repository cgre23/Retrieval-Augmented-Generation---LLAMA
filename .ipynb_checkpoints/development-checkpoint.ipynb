{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf3c56c3-1070-45b3-b032-ba974471d165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.document_loaders import TextLoader, JSONLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "fmt = 'json'\n",
    "# Make sure that you place the OPENAI_API_KEY in the .env file in this folder\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05a02a65-1a70-429d-8987-9b64c38db621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers(fmt):\n",
    "\n",
    "    \"\"\"Fetches papers from the arXiv API and returns them as a list of strings.\"\"\"\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=ti:llama&start=0&max_results=70'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read().decode('utf-8')\n",
    "    root = ET.fromstring(data)\n",
    "    papers_list = []\n",
    "\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry' ):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "        summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "        paper_info = f\"Title: {title} Summary: {summary}\"\n",
    "\n",
    "        papers_list.append(paper_info)\n",
    "    # Check if the list has some content\n",
    "    if fmt == 'json':\n",
    "        dic = {}\n",
    "        dic['content'] = papers_list\n",
    "    \n",
    "        if papers_list != []:\n",
    "            with open('data.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(dic, f)\n",
    "                \n",
    "    if papers_list != [] and fmt == 'txt':\n",
    "        file = open('data.txt','w')\n",
    "        for item in papers_list:\n",
    "            file.write(item)\n",
    "        file.close()\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7226eda9-6ec1-49fe-baee-c84b03d964ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_papers('json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5d9c2-142f-45eb-b68c-e828559e1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file \n",
    "f = open('data.json',) \n",
    "   \n",
    "# returns JSON object as  \n",
    "# a dictionary \n",
    "data = json.load(f) \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "658699cf-890a-4ad4-a389-5920b467c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path=\"./data.json\",\n",
    "    jq_schema='.content[]',\n",
    "    text_content=True)\n",
    "docs = loader.load()\n",
    "\n",
    "#loader = TextLoader(\"./data.txt\")\n",
    "#docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb41ffd-7864-431c-b619-df7d74deeb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "293820ff-914a-4691-857b-e307b6f9b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split text and create vector embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "#prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "#rag_chain = (\n",
    "#    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#    | prompt\n",
    "#    | llm\n",
    "#    | StrOutputParser()\n",
    "#)\n",
    "\n",
    "# RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f32bb003-bac1-42aa-970d-29cba0d0bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Name at least 5 domain-specific LLMs that have been created by fine-tuning Llama-2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "847472c5-02a9-4d6e-aca9-6f1eb5452735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The retrieved context does not provide specific information about the model structure of Llama-2, including the required memory, required computing capacity, number of parameters, or available quantizations.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What can you find out about the model structure of Llama-2 (required memory, required computing capacity, number of parameters, available quantizations)? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9d7f58e-c5bc-4699-8b39-61ae8b083a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-2 has been successfully used for dialogue use cases and has outperformed open-source chat models on most benchmarks tested. Promising areas of application for Llama-2 include dialogue systems and as a substitute for closed-source models based on its helpfulness and safety evaluations.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e6f296b-291f-4741-b82f-0d946d8310a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context does not provide the names of any specific domain-specific LLMs that have been created by fine-tuning Llama-2.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Name at least 5 domain-specific LLMs that have been created by fine-tuning Llama-2.\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb97400-6570-4a97-9950-38ec5cc23c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What can you find out about the model structure of Llama-2 (required memory, required computing capacity, number of parameters, available quantizations)?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47308439-0d93-4905-b468-f45bc4c23d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
